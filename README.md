# Resource-Efficient-Deep-Learning-Hub

Welcome to the Resource-Efficient Deep Learning Hub!  
This repository is dedicated to aggregating milestone papers, open-source libraries, and key resources focused on resource efficiency in computer vision.

---

## üè∑Ô∏è Label Descriptions

Below are the descriptions for each label used in the tables:

- **Novel algorithms**: New methods or architectures proposed to improve resource efficiency.
- **Training paradigms**: Strategies or techniques used during model training to reduce computational cost or memory usage.
- **Learning paradigms**: Approaches to how models learn, such as self-supervised or transfer learning, that impact efficiency.
- **Prompting paradigms**: Methods involving prompts (e.g., in transformers) to guide efficient learning or inference.
- **Hardware optimization techniques**: Approaches that leverage hardware features or design for better resource management.
- **Deployment**: Methods and tools focused on making models efficient for real-world deployment on resource-constrained devices.
- **Pruning**: Techniques for removing unnecessary weights or neurons from neural networks to reduce size and computation.
- **Quantization**: Approaches for reducing the precision of weights and activations to save memory and speed up inference.
- **Knowledge distillation**: Techniques where a smaller model is trained to mimic a larger one, improving efficiency.
- **Clustering**: Methods for grouping similar weights or structures in models to reduce redundancy.

---

## üìÑ Milestone Papers

| Title                                                         | Year | Key Highlights                                             | Task                  | Label(s)                                | [...]
|---------------------------------------------------------------|------|------------------------------------------------------------|-----------------------|-----------------------------------------|-[...]
| MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications | 2017 | Introduces depthwise separable convolutions for reduced computation and memory. | Image Classification  | [...]
| EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks | 2019 | Proposes a compound scaling method for balancing depth, width, and resolution. | Image Classification  | Training pa[...]

---

## üõ†Ô∏è Open Source Libraries/Frameworks

| Name       | Description                                              | Key Highlights                                    | Stars | Label(s)                          | Repository/URL                [...]
|------------|----------------------------------------------------------|---------------------------------------------------|-------|-----------------------------------|-------------------------------[...]
| TensorFlow Model Optimization Toolkit | Tools and APIs to optimize ML models for deployment and efficiency. | Supports quantization, pruning, clustering.       | 3.7k  | Hardware optimization, Train[...]
| Distiller  | Python package for neural network compression research.   | Pruning, quantization, knowledge distillation.    | 2.8k  | Training, Learning paradigms      | [Distiller](https://github.co[...]

---

## üöÄ How to Contribute

- Add new entries to the tables above by filling in the relevant details.
- For papers, include the title, year, key highlights, task addressed, label(s) (e.g., novel algorithms, training, learning, prompting paradigms, hardware optimization techniques), and link to the rep[...]
- For libraries/frameworks, include the name, a short description, main highlights, GitHub stars (if applicable), label(s), and the repository URL.

---

Feel free to expand and customize this template as your collection grows!
