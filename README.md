# Resource-Efficient-Deep-Learning-Hub

Welcome to the Resource-Efficient Deep Learning Hub!  
This repository is dedicated to aggregating milestone papers, open-source libraries, and key resources focused on resource efficiency in computer vision.

---

## üìÑ Milestone Papers

| Title                                                         | Year | Key Highlights                                             | Task                  | Label(s)                                | Repository/URL                                      |
|---------------------------------------------------------------|------|------------------------------------------------------------|-----------------------|-----------------------------------------|-----------------------------------------------------|
| MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications | 2017 | Introduces depthwise separable convolutions for reduced computation and memory. | Image Classification  | Novel algorithms, Hardware optimization | [arXiv:1704.04861](https://arxiv.org/abs/1704.04861)|
| EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks | 2019 | Proposes a compound scaling method for balancing depth, width, and resolution. | Image Classification  | Training paradigms, Novel algorithms    | [arXiv:1905.11946](https://arxiv.org/abs/1905.11946)|

---

## üõ†Ô∏è Open Source Libraries/Frameworks

| Name       | Description                                              | Key Highlights                                    | Stars | Label(s)                          | Repository/URL                                   |
|------------|----------------------------------------------------------|---------------------------------------------------|-------|-----------------------------------|--------------------------------------------------|
| TensorFlow Model Optimization Toolkit | Tools and APIs to optimize ML models for deployment and efficiency. | Supports quantization, pruning, clustering.       | 3.7k  | Hardware optimization, Training   | [TF Model Optimization](https://github.com/tensorflow/model-optimization) |
| Distiller  | Python package for neural network compression research.   | Pruning, quantization, knowledge distillation.    | 2.8k  | Training, Learning paradigms      | [Distiller](https://github.com/IntelLabs/distiller)                      |

---

## üöÄ How to Contribute

- Add new entries to the tables above by filling in the relevant details.
- For papers, include the title, year, key highlights, task addressed, label(s) (e.g., novel algorithms, training, learning, prompting paradigms, hardware optimization techniques), and link to the repository or paper.
- For libraries/frameworks, include the name, a short description, main highlights, GitHub stars (if applicable), label(s), and the repository URL.

---

Feel free to expand and customize this template as your collection grows!
